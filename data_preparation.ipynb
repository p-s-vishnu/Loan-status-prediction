{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train3/train.csv\")\ntest_df = pd.read_csv(\"../input/test-file/test.csv\")\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7041905c340a56ca30197d9edbecdbe30ebeb950"
      },
      "cell_type": "markdown",
      "source": "## Missing value imputation\nLet’s list out feature-wise count of missing values."
    },
    {
      "metadata": {
        "_uuid": "d35b3c051e1e400330364c5ebe03c8cad6193b99"
      },
      "cell_type": "raw",
      "source": "train_df.isnull().sum()"
    },
    {
      "metadata": {
        "_uuid": "a4b8769621815c31f679a8c12ad8f327ecabc147"
      },
      "cell_type": "markdown",
      "source": "We will treat the missing values in all the features one by one.\n\n- **For numerical variables:**  imputation using mean or median\n- **For categorical variables:** imputation using mode"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34df525498f00826061a8c650cfbe14c435d863b"
      },
      "cell_type": "code",
      "source": "train_df['Gender'].fillna(train_df['Gender'].mode()[0], inplace=True)\ntrain_df['Married'].fillna(train_df['Married'].mode()[0], inplace=True)\ntrain_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True)\ntrain_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0], inplace=True)\ntrain_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd384f033871456a9d61debf8acdff43afc0136e"
      },
      "cell_type": "markdown",
      "source": "Now let’s try to find a way to fill the missing values in Loan_Amount_Term. We will look at the value count of the Loan amount term variable."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9d9d70c7680963c6ae7f82156560e7662d3c61d3"
      },
      "cell_type": "code",
      "source": "train_df['Loan_Amount_Term'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bf05fe0df076e8ab4457da850425196cf911af9"
      },
      "cell_type": "markdown",
      "source": "It can be seen that in loan amount term variable, the value of 360 is repeating the most. So we will replace the missing values in this variable using the mode of this variable."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed42d8822e26700691a83678ff161bdcb72e99b3"
      },
      "cell_type": "code",
      "source": "train_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].median(), inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69205103f4004322f95b59738024a81ad93f7a2d"
      },
      "cell_type": "markdown",
      "source": "Now lets check whether all the missing values are filled in the dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5bab65cdde039e0d6c1c4ae6941ec999f6ddd95"
      },
      "cell_type": "code",
      "source": "train_df.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "703904cfd04eaea4176028d3942bbe7bd9303c80"
      },
      "cell_type": "markdown",
      "source": "As we can see that all the missing values have been filled in the test dataset. Let’s fill all the missing values in the test dataset too with the same approach."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95a99153d5a6908e48f3c5524063797958356523"
      },
      "cell_type": "code",
      "source": "test_df['Gender'].fillna(train_df['Gender'].mode()[0], inplace=True)\ntest_df['Dependents'].fillna(train_df['Dependents'].mode()[0], inplace=True)\ntest_df['Self_Employed'].fillna(train_df['Self_Employed'].mode()[0], inplace=True)\ntest_df['Credit_History'].fillna(train_df['Credit_History'].mode()[0], inplace=True)\ntest_df['Loan_Amount_Term'].fillna(train_df['Loan_Amount_Term'].mode()[0], inplace=True)\ntest_df['LoanAmount'].fillna(train_df['LoanAmount'].median(), inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f4019c9d8e7c5fc3ed41a86154576128621ca67a"
      },
      "cell_type": "markdown",
      "source": "## Outlier Treatment"
    },
    {
      "metadata": {
        "_uuid": "776013fd7186e1dd18897938427df6def2b56b59"
      },
      "cell_type": "markdown",
      "source": "As we saw earlier in univariate analysis, LoanAmount contains outliers so we have to treat them as the presence of outliers affects the distribution of the data. Let's examine what can happen to a data set with outliers.\n\nDue to these outliers bulk of the data in the loan amount is at the left and the right tail is longer. This is called right skewness. \n\nOne way to remove the skewness is by doing the **log transformation**. As we take the log transformation, it does not affect the smaller values much, but reduces the larger values. So, we get a distribution similar to normal distribution."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3a5a0e21938dfcb4ad05c1e55531a11bfd567cd"
      },
      "cell_type": "code",
      "source": "train_df['LoanAmount_log'] = np.log(train_df['LoanAmount'])\ntrain_df['LoanAmount_log'].hist(bins=20)\ntest_df['LoanAmount_log'] = np.log(test_df['LoanAmount'])",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}